<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/images/favicon/favicon.ico">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
    
    
        <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png">
    
    
        <link rel="mask-icon" href="/images/favicon/safari-pinned-tab.svg">
    


    <!-- meta -->


<title>NLPCourse | Cactus Ball</title>


    <meta name="keywords" content="ML, NLP">




    <!-- OpenGraph -->
 
    <meta name="description" content="NLP只是复习课程时的笔记，公式渲染好像有问题。 数学基础概率论 全概率公式  $$P(A) &#x3D; \sum_{i&#x3D;1}^n P(B_i)P(A|B_i)$$  贝叶斯  $$P(B_i|A) &#x3D; \frac{P(B_i)P(A|B_i)}{\sum_{j&#x3D;1}^{n}P(B_j)P(A|B_j)}$$ 贝叶斯决策理论，已知$P(x|w_i)$，观察到向量x，可以用贝叶斯算后验概率：$P(w_i|x">
<meta property="og:type" content="article">
<meta property="og:title" content="NLPCourse">
<meta property="og:url" content="https://cactusballl.github.io/2022/07/09/NLPCourse/index.html">
<meta property="og:site_name" content="Cactus Ball">
<meta property="og:description" content="NLP只是复习课程时的笔记，公式渲染好像有问题。 数学基础概率论 全概率公式  $$P(A) &#x3D; \sum_{i&#x3D;1}^n P(B_i)P(A|B_i)$$  贝叶斯  $$P(B_i|A) &#x3D; \frac{P(B_i)P(A|B_i)}{\sum_{j&#x3D;1}^{n}P(B_j)P(A|B_j)}$$ 贝叶斯决策理论，已知$P(x|w_i)$，观察到向量x，可以用贝叶斯算后验概率：$P(w_i|x">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-07-09T08:49:51.000Z">
<meta property="article:modified_time" content="2022-07-09T08:55:12.399Z">
<meta property="article:author" content="cactus ball">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
        <link rel="stylesheet" id="hl-default-theme" href="/css/highlight/default.css" media="none" >
        
            <link rel="stylesheet" id="hl-dark-theme" href="/css/highlight/dark.css" media="none">
        
    

    
    

    
    
<link rel="stylesheet" href="/css/style/dark.css">

    
<script src="/js/darkmode.js"></script>



     

    <!-- custom head -->

<meta name="generator" content="Hexo 5.4.0"></head>

    <body>
        <div id="app">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">aaicy64&#39;s Blog</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">首页</a>
                
                    <a href="/tags/" class="navbar-menu button">标签</a>
                
                    <a href="/archives/" class="navbar-menu button">归档</a>
                
                    <a href="/friends/" class="navbar-menu button">友链</a>
                
            </div>
        
        
        

        
        
    <a href="javaScript:void(0);" id="btn-toggle-dark">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">首页</a>
                
                    <a href="/tags/" class="dropdown-menu button">标签</a>
                
                    <a href="/archives/" class="dropdown-menu button">归档</a>
                
                    <a href="/friends/" class="dropdown-menu button">友链</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        NLPCourse
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2022/07/" class="post-meta__date button">2022-07-09</a>
        
 
        
    
    


 

 
    </div>
</div>



<article class="post content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p>只是复习课程时的笔记，公式渲染好像有问题。</p>
<h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><h3 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h3><ul>
<li>全概率公式</li>
</ul>
<p>$$<br>P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)<br>$$</p>
<ul>
<li>贝叶斯</li>
</ul>
<p>$$<br>P(B_i|A) = \frac{P(B_i)P(A|B_i)}{\sum_{j=1}^{n}P(B_j)P(A|B_j)}<br>$$</p>
<p>贝叶斯决策理论，已知$P(x|w_i)$，观察到向量x，可以用贝叶斯算后验概率：$P(w_i|x)$</p>
<ul>
<li>基于最小错误率的贝叶斯决策</li>
</ul>
<p>$$<br>P(w_i|x) = \max_{j=1,2,…,c}P(w_j|x),则判断x\in{w_i}<br>$$</p>
<h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><ul>
<li>熵</li>
</ul>
<p>X是离散型随机变量，<br>$$<br>H(X) = - \sum_{x \in X} p(x) \log_2p(x)<br>$$<br>规定 0log0 = 0。</p>
<ul>
<li>联合熵</li>
</ul>
<p>$$<br>H(X,Y) = - \sum_{x\in X}\sum_{y\in Y}p(x,y)\log_2p(x,y)<br>$$</p>
<ul>
<li>条件熵</li>
</ul>
<p>$$<br>H(Y|X) = \sum_{x\in X}p(x)H(Y|X = x) \<br>=\sum_{x\in X}p(x)[-\sum_{y\in Y}p(y|x)\log_2 p(y|x)] \<br>= - \sum_{x\in X}\sum_{y\in Y}p(x,y)\log_2p(y|x)<br>$$</p>
<ul>
<li>连锁规则</li>
</ul>
<p>$$<br>H(X,Y) = H(X) + H(Y|X)<br>$$</p>
<ul>
<li>熵率</li>
</ul>
<p>$$<br>H_{rate} = \frac{1}{n}H(X_{1n})<br>$$</p>
<p>平均每个字符的熵</p>
<ul>
<li>相对熵</li>
</ul>
<p>$$<br>D(p||q) = \sum_{x \in X}p(x)\log{\frac{p(x)}{q(x)}}<br>$$</p>
<p>0log(0/q) = 0,plog(p/0) = $\infin$​</p>
<p>相对熵常被用以衡量两个随机分布的差距。</p>
<ul>
<li>交叉熵</li>
</ul>
<p>交叉熵的概念用以衡量估计模型与真实概率分布之间的差异。<br>$$<br>X \sim  p(x) \ \ \ q(x)近似p(x)\<br>H(X,q) = H(X) + D(p||q) = - \sum_x p(x)\log q(x)<br>$$<br>假设这种语言是“理想”的，即 𝑛 趋于无穷大时，其全部“单词”的概率之和为1。<br>$$<br>H(L,q) = -\lim_{n \rarr \infin}\frac{1}{n}\log q(x_1^n)<br>$$<br>由此，我们可以根据模型 𝑞 和一个含有大量数据的 𝐿的样本来计算交叉熵。在设计模型 𝑞 时，我们的目的是使交叉熵最小，从而使模型最接近真实的概率分布 𝑝(𝑥)。</p>
<ul>
<li>困惑度</li>
</ul>
<p>$$<br>PP_q = 2^{H(L,q)} \approx 2 ^ {-\frac{1}{n}q(l_1^n)} = [q(l_1^n)]^{-\frac{1}{n}}<br>$$</p>
<ul>
<li>互信息</li>
</ul>
<p>$$<br>I(X;Y) = H(X) - H(X|Y) \<br>= \sum_{x \in X}\sum_{y \in Y}p(x,y)\log_2 \frac{p(x,y)}{p(x)p(y)}<br>$$</p>
<p>互信息 𝐼(𝑋; 𝑌) 是在知道了 𝑌 的值以后 𝑋 的不确定性的减少量，即 𝑌 的值透露了多少关于 𝑋 的信息量。</p>
<p>在判断两个连续汉字之间的结合强度方面，双字耦合度要比互信息更合适一些。</p>
<h3 id="噪声信道模型"><a href="#噪声信道模型" class="headerlink" title="噪声信道模型"></a>噪声信道模型</h3><p>信道容量 $C = \max_{p(x)} I(X;Y)$</p>
<p>基于上下文分类的词义消除歧义。贝叶斯。</p>
<p>基于最大熵的消歧方法。</p>
<h3 id="形式语言与自动机"><a href="#形式语言与自动机" class="headerlink" title="形式语言与自动机"></a>形式语言与自动机</h3><p>$G = (N,\Sigma,P,S)$ ,N是非终结符的有限集合，Σ 是终结符的有限集合，𝑁 ∩ Σ = Φ；𝑉 = 𝑁 ∪ Σ 称为总词汇表；𝑃 是一组重写规则的有限集合，𝑆 ∈ 𝑁，称为句子符或初始符。</p>
<p>约定每步推导中只改写最左边的那个非终结符，这种推导称为“最左推导”。<br>约定每步推导中只改写最右边的那个非终结符，这种推导称为“最右推导”。<br>最右推导也称规范推导。</p>
<h4 id="文法分类"><a href="#文法分类" class="headerlink" title="文法分类"></a>文法分类</h4><p>在乔姆斯基的语法理论中，文法被划分为4种类型：<br> 3型文法——正则文法(有限自动机)<br>2型文法——上下文无关文法（下推自动机）<br>1型文法——上下文有关文法（线性带限自动机）<br>0型文法——无约束文法（图灵机）</p>
<p>限制依次减少。<br>$$<br>L(G3) \sub L(G2) \sub L(G1) \sub L(G0)<br>$$</p>
<h4 id="DFA"><a href="#DFA" class="headerlink" title="DFA"></a>DFA</h4><p>确定的有限自动机 𝑀 是一个五元组：<br>𝑀 = （Σ, 𝑄, 𝛿, 𝑞<del>0</del>, 𝐹）<br>其中， Σ 是输入符号的有穷集合；<br>𝑄 是状态的有限集合；<br>𝑞<del>0</del> ∈ 𝑄 是初始状态；<br>𝐹 是终止状态集合，𝐹 ⊆ 𝑄；<br>𝛿 是 𝑄 与 Σ 的直积 𝑄 × Σ 到 𝑄 (下一个状态) 的映射，<br>支配着有限状态控制的行为，有时也称为状态转移函数。</p>
<h4 id="NFA"><a href="#NFA" class="headerlink" title="NFA"></a>NFA</h4><p>不确定性有限自动机<br>(non-definite automata, NFA)<br>不确定性有限自动机 𝑀 是一个五元组：<br>𝑀 = Σ, 𝑄, 𝛿, 𝑞0, 𝐹<br>其中， Σ 是输入符号的有穷集合；<br>𝑄 是状态的有限集合；<br>𝑞0 ∈ 𝑄 是初始状态；<br>𝐹 是终止状态集合，𝐹 ⊆ 𝑄；<br>𝛿 是 𝑄 与 Σ 的直积 𝑄 × Σ 到 𝑄 的幂集 2𝑄 的映射。</p>
<h4 id="PDA"><a href="#PDA" class="headerlink" title="PDA"></a>PDA</h4><p>一个不确定的PDA可以表达成一个7元组：<br>𝑀 = (Σ, 𝑄, Γ, 𝛿, 𝑞0, 𝑍0, 𝐹)<br>其中， Σ 是输入符号的有穷集合；<br>𝑄 是状态的有限集合；𝑞0 ∈ 𝑄 是初始状态；<br>Γ 为下推存储器符号的有穷集合；<br>𝑍0 ∈ Γ 为最初出现在下推存储器顶端的符号;<br>𝐹 是终止状态集合，𝐹 ⊆ 𝑄；<br>𝛿 是从 𝑄 × Σ × 𝜀 × Γ 到 𝑄 × Γ∗ 子集的映射。</p>
<h4 id="英语单词拼写检查"><a href="#英语单词拼写检查" class="headerlink" title="英语单词拼写检查"></a>英语单词拼写检查</h4><ul>
<li>编辑距离：𝑒𝑑(𝑋[𝑚], 𝑌[𝑛]) 定义为：从字符串 𝑋 转换到 𝑌 需<br>要的插入、删除、替换和交换两个相邻的基本单位(字符)<br>的最小个数</li>
</ul>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>𝑤𝑖 的 概 率 由 𝑤1, ⋯ , 𝑤𝑖−1 决 定 ， 由 特 定 的 一 组<br>𝑤1, ⋯ , 𝑤𝑖−1 构 成 的 一 个 序 列 ， 称 为 𝑤𝑖 的 历 史<br>（history）。</p>
<h3 id="n元文法"><a href="#n元文法" class="headerlink" title="n元文法"></a>n元文法</h3><p>历史取最近几个词（划分等价类），独立（unigram），马尔科夫链。</p>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>$$<br>p(w_i|w_{history})<br>$$</p>
<p>可由训练语料极大似然估计。</p>
<p>数据稀疏引起零概率问题 =&gt; 数据平滑。</p>
<h3 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h3><p>目标：min困惑度</p>
<p>约束：概率和为1</p>
<ul>
<li>加1法</li>
</ul>
<p>分子加1</p>
<ul>
<li>减值法/折扣法</li>
</ul>
<p>修改训练样本中事件的实际计数，使样本中(实际出现的)不同事件的概率之和小于1，剩余的概率量分配给未见事件。</p>
<h4 id="Good-Turing估计"><a href="#Good-Turing估计" class="headerlink" title="Good-Turing估计"></a>Good-Turing估计</h4><p>$$<br>r^* = (r + 1)\frac{n_{r+1}}{n_r} \<br>p_r = \frac{r^*}{N} \<br>\sum_{r&gt;0}n_r \times p_r = 1 - \frac{n_1}{N} &lt; 1<br>$$</p>
<p>$\frac{n_1}{N}$均分给未出现的事件。</p>
<p>Good-Turing 估计适用于大词汇集产生的符合多项式分布的大量的观察数据。</p>
<h4 id="减值法-折扣——Back-off-后备-后退-方法"><a href="#减值法-折扣——Back-off-后备-后退-方法" class="headerlink" title="减值法/折扣——Back-off (后备/后退)方法"></a>减值法/折扣——Back-off (后备/后退)方法</h4><h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><p>Good-Turing法：对非0事件按公式削减出现的次数，<br>余留出来的概率均分给0概率事件。<br>Katz后退法：对非0事件按Good-Turing法计算减值，<br>余留出来的概率按低阶分布分给0概率事件。<br>绝对减值法：对非0事件无条件削减某一固定的出现<br>次数值，余留出来的概率均分给0概率事件。<br>线性减值法：对非0事件根据出现次数按比例削减次<br>数值，余留出来的概率均分给0概率事件。</p>
<h4 id="删除插值法"><a href="#删除插值法" class="headerlink" title="删除插值法"></a>删除插值法</h4><p>$$<br>p(w_3|w_1w_2) = \lambda_3p’(w_3|w_1w_2) + \lambda_2p’(w_3|w_2) + \lambda_1p’(w_3) \<br>\lambda_1 + \lambda_2 + \lambda_3 = 1<br>$$</p>
<p>将训练语料分为两部分，即从原始语料中删除一部分<br>作为留存数据(holdout data)。<br>① 第一部分用于估计 𝑝′ 𝑤3 𝑤1𝑤2 , 𝑝′ 𝑤3 𝑤2 和 𝑝′ 𝑤3 。<br>② 第二部分用于计算 𝜆1, 𝜆2, 𝜆3 ：使语言模型对留存数<br>据的困惑度最小。</p>
<h3 id="语言模型的自适应"><a href="#语言模型的自适应" class="headerlink" title="语言模型的自适应"></a>语言模型的自适应</h3><ul>
<li>基于缓存的语言模型 (cache-based LM)</li>
<li>基于混合方法的语言模型</li>
<li>基于最大熵的语言模型</li>
</ul>
<h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><p> 𝑏𝑗(𝑘) 为实验员从第 𝑗 个袋子中取出第 𝑘 种颜<br>色的球的概率。<br>$$<br>\begin{cases}<br>    b_j(k) = p(O_t = v_k|q_t = S_j)\<br>    b_j(k) \ge 0 \<br>    \sum_{k = 1}^M b_j(k) = 1<br>\end{cases}<br>$$<br>从初始状态的概率分布为：𝜋 = 𝜋𝑖</p>
<p>为了方便 ，一般将 HMM记为：𝜇 = (𝐴, 𝐵, 𝜋) 或 者𝜇 = (𝑆, 𝑂, 𝐴, 𝐵, 𝜋) 用以指出模型的参数集合。</p>
<p>A 是状态转移概率矩阵，B是概率分布矩阵，𝜋是初始概率分布。</p>
<h4 id="前向变量"><a href="#前向变量" class="headerlink" title="前向变量"></a>前向变量</h4><p>$$<br>\alpha_t(i) = p(O_1O_2…O_t,q_t = S_i|\mu) \<br>\beta_t(i) = p(O_{t+1}O_{t+2}…O_T|q_t = S_i,\mu)<br>$$</p>
<h4 id="Viterbi搜索算法"><a href="#Viterbi搜索算法" class="headerlink" title="Viterbi搜索算法"></a>Viterbi搜索算法</h4><p>$$<br>p(q_t = S_i,O|\mu) = \alpha_t(i) \times \beta_t(i)\<br>\delta_{t+1} = \max_j[\delta_{t}(j) \times a_{ji}]\times b_i(O_{t+1})<br>$$</p>

    </div>
     
    <div class="post-footer__meta"><p>updated at 2022-07-09</p></div> 
    <div class="post-entry__tags"><a href="/tags/ML/" class="post-tags__link button"># ML</a><a href="/tags/NLP/" class="post-tags__link button"># NLP</a></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
        </div>
        <div class="nav__next">
            
                <a href="/2022/07/09/RubyPL/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            Next Post
                        </div>
                        <div class="nav__title">
                            RubyPL
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
     
    <a href="#" class="button" id="b2t" aria-label="Back to Top" title="Back to Top">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>

    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2022 <a href="/">Cactus Ball</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

 



 



 


    
 

 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: {'[+]': [['$', '$']]},
                    tags: 'ams'
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 

 




    </body>
</html>
